{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist               object\n",
      "track_id             object\n",
      "track                object\n",
      "owner                object\n",
      "artist&track         object\n",
      "danceability        float64\n",
      "energy              float64\n",
      "key                   int64\n",
      "loudness            float64\n",
      "mode                  int64\n",
      "speechiness         float64\n",
      "acousticness        float64\n",
      "instrumentalness    float64\n",
      "liveness            float64\n",
      "valence             float64\n",
      "tempo               float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": "              artist                track_id           track  \\\n0         Romanthony  1iwZVgKv3FKc0dqhnSG9uW  Get It 2 Getta   \n1  Glenn Underground  2JFeAyJLMPPsBkklQEet6t         H-Dance   \n2          DJ BORING  3ilkEyg6OCtd9qCnOJkPzU          Winona   \n3          DJ BORING  3ilkEyg6OCtd9qCnOJkPzU          Winona   \n4          DJ BORING  3ilkEyg6OCtd9qCnOJkPzU          Winona   \n\n                       owner               artist&track  danceability  energy  \\\n0  kv718oiku8q612q0zi4iaovzb  Romanthony/Get It 2 Getta         0.828   0.338   \n1  kv718oiku8q612q0zi4iaovzb  Glenn Underground/H-Dance         0.696   0.524   \n2  kv718oiku8q612q0zi4iaovzb           DJ BORING/Winona         0.656   0.693   \n3  botetauw3lk6anvygizm8wolf           DJ BORING/Winona         0.656   0.693   \n4              thebootlegboy           DJ BORING/Winona         0.656   0.693   \n\n   key  loudness  mode  speechiness  acousticness  instrumentalness  liveness  \\\n0    5   -17.362     0       0.1510      0.038000             0.804    0.0422   \n1    0   -16.110     1       0.0489      0.000879             0.252    0.1040   \n2    5    -7.876     0       0.0703      0.038700             0.830    0.1110   \n3    5    -7.876     0       0.0703      0.038700             0.830    0.1110   \n4    5    -7.876     0       0.0703      0.038700             0.830    0.1110   \n\n   valence    tempo  \n0    0.805  120.525  \n1    0.672  122.015  \n2    0.158  122.028  \n3    0.158  122.028  \n4    0.158  122.028  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>artist</th>\n      <th>track_id</th>\n      <th>track</th>\n      <th>owner</th>\n      <th>artist&amp;track</th>\n      <th>danceability</th>\n      <th>energy</th>\n      <th>key</th>\n      <th>loudness</th>\n      <th>mode</th>\n      <th>speechiness</th>\n      <th>acousticness</th>\n      <th>instrumentalness</th>\n      <th>liveness</th>\n      <th>valence</th>\n      <th>tempo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Romanthony</td>\n      <td>1iwZVgKv3FKc0dqhnSG9uW</td>\n      <td>Get It 2 Getta</td>\n      <td>kv718oiku8q612q0zi4iaovzb</td>\n      <td>Romanthony/Get It 2 Getta</td>\n      <td>0.828</td>\n      <td>0.338</td>\n      <td>5</td>\n      <td>-17.362</td>\n      <td>0</td>\n      <td>0.1510</td>\n      <td>0.038000</td>\n      <td>0.804</td>\n      <td>0.0422</td>\n      <td>0.805</td>\n      <td>120.525</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Glenn Underground</td>\n      <td>2JFeAyJLMPPsBkklQEet6t</td>\n      <td>H-Dance</td>\n      <td>kv718oiku8q612q0zi4iaovzb</td>\n      <td>Glenn Underground/H-Dance</td>\n      <td>0.696</td>\n      <td>0.524</td>\n      <td>0</td>\n      <td>-16.110</td>\n      <td>1</td>\n      <td>0.0489</td>\n      <td>0.000879</td>\n      <td>0.252</td>\n      <td>0.1040</td>\n      <td>0.672</td>\n      <td>122.015</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DJ BORING</td>\n      <td>3ilkEyg6OCtd9qCnOJkPzU</td>\n      <td>Winona</td>\n      <td>kv718oiku8q612q0zi4iaovzb</td>\n      <td>DJ BORING/Winona</td>\n      <td>0.656</td>\n      <td>0.693</td>\n      <td>5</td>\n      <td>-7.876</td>\n      <td>0</td>\n      <td>0.0703</td>\n      <td>0.038700</td>\n      <td>0.830</td>\n      <td>0.1110</td>\n      <td>0.158</td>\n      <td>122.028</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DJ BORING</td>\n      <td>3ilkEyg6OCtd9qCnOJkPzU</td>\n      <td>Winona</td>\n      <td>botetauw3lk6anvygizm8wolf</td>\n      <td>DJ BORING/Winona</td>\n      <td>0.656</td>\n      <td>0.693</td>\n      <td>5</td>\n      <td>-7.876</td>\n      <td>0</td>\n      <td>0.0703</td>\n      <td>0.038700</td>\n      <td>0.830</td>\n      <td>0.1110</td>\n      <td>0.158</td>\n      <td>122.028</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DJ BORING</td>\n      <td>3ilkEyg6OCtd9qCnOJkPzU</td>\n      <td>Winona</td>\n      <td>thebootlegboy</td>\n      <td>DJ BORING/Winona</td>\n      <td>0.656</td>\n      <td>0.693</td>\n      <td>5</td>\n      <td>-7.876</td>\n      <td>0</td>\n      <td>0.0703</td>\n      <td>0.038700</td>\n      <td>0.830</td>\n      <td>0.1110</td>\n      <td>0.158</td>\n      <td>122.028</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# database is small enough to be read into memory hence we will use pandas and then cast it into a tf.data object\n",
    "data = pd.read_csv('./data/full_data.csv',index_col=0)\n",
    "data = data.dropna()\n",
    "pprint(data.dtypes)\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train/val/test split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "float_cols = [col for col in data.columns if data[col].dtype == 'float64']\n",
    "all_cols = ['owner', 'artist&track'] + float_cols"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "df = data[all_cols]\n",
    "train, test = train_test_split(df, test_size=0.15, random_state=21)\n",
    "train, val = train_test_split(train, test_size=0.15, random_state=21)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For EDA of the datasets inspect ``dense.ipynb``"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "tf_conv = lambda x: tf.data.Dataset.from_tensor_slices(dict(x))\n",
    "train_tf = tf_conv(train)\n",
    "val_tf = tf_conv(val)\n",
    "test_tf = tf_conv(test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "data": {
      "text/plain": "(111490, 10)"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_df = df.copy()\n",
    "tracks_df = tracks_df.drop('owner', axis=1)\n",
    "tracks_df = tracks_df.drop_duplicates()\n",
    "tracks_df.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "unique_track_names = df['artist&track'].unique()\n",
    "unique_user_ids = df['owner'].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 20:27:20.668795: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'artist&track': <tf.Tensor: shape=(), dtype=string, numpy=b'Romanthony/Get It 2 Getta'>,\n 'danceability': <tf.Tensor: shape=(), dtype=float64, numpy=0.828>,\n 'energy': <tf.Tensor: shape=(), dtype=float64, numpy=0.338>,\n 'loudness': <tf.Tensor: shape=(), dtype=float64, numpy=-17.362>,\n 'speechiness': <tf.Tensor: shape=(), dtype=float64, numpy=0.151>,\n 'acousticness': <tf.Tensor: shape=(), dtype=float64, numpy=0.038>,\n 'instrumentalness': <tf.Tensor: shape=(), dtype=float64, numpy=0.804>,\n 'liveness': <tf.Tensor: shape=(), dtype=float64, numpy=0.0422>,\n 'valence': <tf.Tensor: shape=(), dtype=float64, numpy=0.805>,\n 'tempo': <tf.Tensor: shape=(), dtype=float64, numpy=120.525>}"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks = tf.data.Dataset.from_tensor_slices(dict(tracks_df))\n",
    "next(iter(tracks))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "tracks1 = tf.data.Dataset.from_tensor_slices({'artist&track': unique_track_names})\n",
    "# tracks1 = tf.data.Dataset.from_tensors(unique_track_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 20:26:48.653012: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'artist&track': <tf.Tensor: shape=(), dtype=string, numpy=b'Romanthony/Get It 2 Getta'>}"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(tracks1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Two tower model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Query (user) model\n",
    "\n",
    "A simple query model that involves a `tf.keras.layers.Embedding` layer and a stack of dense layers with a ReLu activation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, embedding_dimension, deep_layer_sizes):\n",
    "        super().__init__()\n",
    "        self._embedding_dimension = embedding_dimension\n",
    "\n",
    "        self.user_embedding: tf.keras.layers.Layer = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=unique_user_ids, mask_token=None, name='user_string'),\n",
    "            tf.keras.layers.Embedding(len(unique_user_ids) + 1, self._embedding_dimension),\n",
    "        ])\n",
    "\n",
    "        self._deep_layers = [tf.keras.layers.Dense(layer_size, activation='relu') for layer_size in deep_layer_sizes]\n",
    "\n",
    "        # final dense layer must have size of the embedding dimension as it's imperative for the matrix factorisation with the\n",
    "        # candidate matrix which has size (num_candidates, embedding_dimension)\n",
    "        self._dense_final_layer = tf.keras.layers.Dense(self._embedding_dimension, activation='relu')\n",
    "\n",
    "    def call(self, input_users):\n",
    "\n",
    "        x = self.user_embedding(input_users)\n",
    "\n",
    "        for deep_layer in self._deep_layers:\n",
    "            x = deep_layer(x)\n",
    "\n",
    "        return self._dense_final_layer(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Candidate Model\n",
    "\n",
    "The main feature for the candidate model is the `artist&track` field which seres as a unique identifier of a song. In the candidate model a `tf.keras.layers.Embedding` layer is used to convert the string into a vector of length `embedding_dimension`.\n",
    "\n",
    "Additional audio features are leveraged to improve accuracy of the retrieval task. This is done by concatenating normalised audio features with the embedded vector and then passing the resultant tensor into a stack of dense layers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "class CandidateModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, embedding_dimension=32, deep_layer_sizes=[256]):\n",
    "        super().__init__()\n",
    "        self._embedding_dimension = embedding_dimension\n",
    "        # audio cols that will be used in the concatenation with the embedded artist&song field\n",
    "        self._audio_cols = ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n",
    "\n",
    "\n",
    "        self.track_embedding: tf.keras.layers.Layer = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=unique_track_names, mask_token=None, name='track_lookup'),\n",
    "            tf.keras.layers.Embedding(len(unique_track_names) + 1, self._embedding_dimension),\n",
    "        ])\n",
    "\n",
    "        self._audio_norm: tf.keras.layers.Layer = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self._deep_layers = [tf.keras.layers.Dense(layer_size, activation='relu') for layer_size in deep_layer_sizes]\n",
    "\n",
    "        # final dense layer must have size of the embedding dimension as it's imperative for the matrix factorisation with the\n",
    "        # query matrix which has size (num_query, embedding_dimension)\n",
    "        self._dense_final_layer = tf.keras.layers.Dense(self._embedding_dimension, activation='relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        tracks_embeddings = self.track_embedding(inputs[\"artist&track\"])\n",
    "\n",
    "        audio = tf.concat([tf.expand_dims(inputs[col],-1) for col in self._audio_cols], axis=1)\n",
    "        audio = self._audio_norm(audio)\n",
    "\n",
    "        x = tf.concat([tracks_embeddings, audio], axis=1)\n",
    "        # x = tracks_embeddings\n",
    "\n",
    "        for deep_layer in self._deep_layers:\n",
    "            x = deep_layer(x)\n",
    "\n",
    "        return self._dense_final_layer(x)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full retrieval model\n",
    "\n",
    "The query and candidate towers are combined here in the full retrieval model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [
    "class RetrievalModel(tfrs.Model):\n",
    "\n",
    "    def __init__(self, embedding_dimension=32, query_dense_list=[256], candidate_dense_list=[256]):\n",
    "        super().__init__()\n",
    "        self._embedding_dimension: int = embedding_dimension\n",
    "        self._query_dense_list: List[int] = query_dense_list\n",
    "        self._candidate_dense_list: List[int] = candidate_dense_list\n",
    "\n",
    "        self.query_model: tf.keras.Model = UserModel(self._embedding_dimension, self._query_dense_list)\n",
    "\n",
    "        self.candidate_model: tf.keras.Model = CandidateModel(self._embedding_dimension, self._candidate_dense_list)\n",
    "\n",
    "        self.task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=tracks.batch(128).map(self.candidate_model),\n",
    "                ks=(50,100),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features, training=False) -> tf.Tensor:\n",
    "        # We pick out the user features and pass them into the user model.\n",
    "        user_embeddings = self.query_model(features[\"owner\"])\n",
    "        # And pick out the track features and pass them into the track model\n",
    "        positive_track_embeddings = self.candidate_model(features)\n",
    "        # The task computes the loss and the metrics.\n",
    "        return self.task(user_embeddings, positive_track_embeddings, compute_metrics=not training)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "57/57 [==============================] - 8s 124ms/step - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 21011.1007 - regularization_loss: 0.0000e+00 - total_loss: 21011.1007\n",
      "Epoch 2/3\n",
      "57/57 [==============================] - 5s 96ms/step - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 15590.2642 - regularization_loss: 0.0000e+00 - total_loss: 15590.2642\n",
      "Epoch 3/3\n",
      "57/57 [==============================] - 5s 89ms/step - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 15547.1936 - regularization_loss: 0.0000e+00 - total_loss: 15547.1936\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7fa04ba82850>"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RetrievalModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "train_cached = train_tf.batch(2048).cache()\n",
    "model.fit(train_cached, epochs=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 213s 5s/step - factorized_top_k/top_50_categorical_accuracy: 0.8932 - factorized_top_k/top_100_categorical_accuracy: 0.8934 - loss: 3058.2988 - regularization_loss: 0.0000e+00 - total_loss: 3058.2988\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.8931623697280884, 0.8933566212654114, 526.74951171875, 0, 526.74951171875]"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_cached = val_tf.batch(512).cache()\n",
    "model.evaluate(val_cached)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 478s 8s/step - factorized_top_k/top_50_categorical_accuracy: 0.8950 - factorized_top_k/top_100_categorical_accuracy: 0.8955 - loss: 15438.0879 - regularization_loss: 0.0000e+00 - total_loss: 15438.0879\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.8949984312057495, 0.8955298066139221, 14964.134765625, 0, 14964.134765625]"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_cached)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Very good results for the both the validation and training sets. A big improvement on the trivial embedding model."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
